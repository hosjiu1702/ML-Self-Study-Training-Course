{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zeros Initialization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 2-layer neural network:\n",
    "\n",
    "* Input layer (layer 1): 2 nodes + 1 bias\n",
    "* Hidden layer (layer 2): 3 nodes + 1 bias\n",
    "* Output layer (layer 3): 2 output nodes\n",
    "\n",
    "![](fig/nn_5.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tổng quát, ta có:\n",
    "\n",
    "* $w_{ij}^{(1)} = m = const, \\enspace \\forall i,j \\enspace(*)$\n",
    "* $w_{ij}^{(2)} = n = const \\enspace \\forall i,j \\enspace(*)$\n",
    "* Consider a training example: $\\{x_{0}, x_{1}, x_{2}\\}$ has $x_{0}$ is bias $(x_{0}=1)$\n",
    "* Cost function (Quadratic cost - MSE): $J = \\frac{1}{2p}\\sum_{j=1}^{j=p}\\sum_{i=1}^{i=s}\\big(y_{i}^{(j)} - a_{i}^{(j)}\\big)^{2}$\n",
    "* Activation function: $g\\big(z\\big) = \\frac{1}{1 + e^{-z}}$\n",
    "* $g'\\big(z\\big) = g\\big(z\\big)\\big(1 - g\\big(z\\big)\\big)$\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "#### Layer 1\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "z_{1}^{(2)} = w_{10}^{(1)}x_{0} + w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} \\\\\n",
    "z_{2}^{(2)} = w_{20}^{(1)}x_{0} + w_{21}^{(1)}x_{1} + w_{22}^{(1)}x_{2} \\\\\n",
    "z_{3}^{(2)} = w_{30}^{(1)}x_{0} + w_{31}^{(1)}x_{1} + w_{32}^{(1)}x_{2}\n",
    "\\end{cases} \\\\\n",
    "$\n",
    "\n",
    "$\n",
    "\\iff (1)\n",
    "\\begin{cases}\n",
    "z_{1}^{(2)} = w_{10}^{(1)}a_{0}^{(1)} + w_{11}^{(1)}a_{1}^{(1)} + w_{12}^{(1)}a_{2}^{(2)} \\\\\n",
    "z_{2}^{(2)} = w_{20}^{(1)}a_{0}^{(1)} + w_{21}^{(1)}a_{1}^{(1)} + w_{22}^{(1)}a_{2}^{(2)} \\\\\n",
    "z_{3}^{(2)} = w_{30}^{(1)}a_{0}^{(1)} + w_{31}^{(1)}a_{1}^{(1)} + w_{32}^{(1)}a_{2}^{(2)}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "$(*) \\enspace and \\enspace (1) \\Rightarrow\n",
    "\\begin{cases}\n",
    "z_{1}^{(2)} = m(x_{0} + x_{1} + x_{2}) \\\\\n",
    "z_{2}^{(2)} = m(x_{0} + x_{1} + x_{2}) \\\\\n",
    "z_{3}^{(2)} = m(x_{0} + x_{1} + x_{2}) \n",
    "\\end{cases} \\\\\n",
    "\\Rightarrow z_{1}^{(2)} = z_{2}^{(2)} = z_{3}^{(3)} = m\\big(\\sum x_{i}\\big) \\\\\n",
    "\\Rightarrow a_{1}^{(2)} = a_{2}^{(2)} = a_{3}^{(2)} = g\\big(m\\big(\\sum x_{i} \\big)\\big) = k = const\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{cases}\n",
    "z_{1}^{(3)} = w_{10}^{(2)}a_{0}^{(2)} + w_{11}^{(2)}a_{1}^{(2)} + w_{12}^{(2)}a_{2}^{(2)} + w_{13}^{(2)}a_{3}^{(2)} \\\\\n",
    "z_{2}^{(3)} = w_{20}^{(2)}a_{0}^{(2)} + w_{21}^{(2)}a_{1}^{(2)} + w_{22}^{(2)}a_{2}^{(2)} +  w_{22}^{(2)}a_{3}^{(2)}\n",
    "\\end{cases} \\\\\n",
    "\\iff\n",
    "\\begin{cases}\n",
    "z_{1}^{(3)} = n + nk + nk + nk \\\\\n",
    "z_{2}^{(3)} = n + nk + nk + nk\n",
    "\\end{cases} \\\\\n",
    "\\iff\n",
    "\\begin{cases}\n",
    "z_{1}^{(3)} = n(1+3k) \\\\\n",
    "z_{2}^{(3)} = n(1+3k)\n",
    "\\end{cases} \\\\\n",
    "\\Rightarrow z_{1}^{(3)} = z_{2}^{(3)} = n(1+3k) \\\\\n",
    "\\Rightarrow a_{1}^{(3)} = a_{2}^{(3)} = g\\big(n(1+3k)\\big) = q = const\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivation of cost function with respect to all weights at layer 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "$\n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(2)}} = \\delta_{i}^{(3)}a_{j}^{(2)} = \n",
    "\\big[\\big(a_{i}^{(3)} - y_{i}\\big)g'\\big(z_{i}^{(3)}\\big)\\big]k = \n",
    "\\big[\\big(q-y_{i}\\big)q\\big(1-q\\big)\\big]k\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\Rightarrow \\frac{\\partial J}{\\partial w_{ij^{(2)}}} = \n",
    "kq\\big(1-q\\big)\\big(q-y_{i}\\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ đây ta có thể suy ra được là các node ẩn *(hidden layer)* có cùng kết nối với node output *(output layer)* **i** thì trọng số sẽ được cập nhật như nhau.\n",
    "\n",
    "Điều này có thể được hiểu như là đối với một node nào đó trong ouput layer sẽ \"nhìn\" các nodes ở lớp liền trước đó như nhau. Và nó đánh mất đi khả năng **representation** của NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "$\n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(1)}} =\n",
    "\\delta_{i}^{(2)}a_{j}^{(1)} = \\delta_{i}^{(2)}x_{j}^{(1)} = \n",
    "\\sum_{j}\\delta_{j}^{(3)}w_{ji}^{(2)} = \n",
    "\\sum_{j}\\Big\\{\\big[\\big(a_{j}^{(3)} - y_{j}\\big)g'\\big(z_{j}^{(3)}\\big)\\big]n\\Big\\} = \n",
    "n\\sum_{j}q\\big(1-q\\big)\\big(q-y_{j}\\big) = \n",
    "nq\\big(1-q\\big)\\sum_{j}\\big(q-y_{j}\\big) \\enspace (2)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bởi vì n = 0 (zeros initialization) nên từ biểu thức (2) suy ra:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(1)}} = 0\n",
    "$$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suy ra các params ở lớp đầu tiên không cập nhật sau mỗi training example.\n",
    "\n",
    "Và điều này cũng tương tự cho $p-1$ examples còn lại (**p: number of training examples**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ở trên ta chỉ xét cho NN 2-layer nhưng cũng có thể tổng quát cho NN $k$-layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vậy, khi khởi tạo các parameters cùng bằng nhau mà cụ thể trong trường hợp này là `zero` thì các trọng số ở layer 1 sẽ không \"learning\" và nó sẽ ảnh hưởng đến các layer sau. Điều này dẫn đến NN sẽ đánh mất đi khả năng vốn có của nó là **representation** mà các linear classifier khác không có.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/\n",
    "* https://www.jeremyjordan.me/neural-networks-training/\n",
    "* https://en.wikipedia.org/wiki/Backpropagation\n",
    "* http://www.deeplearningbook.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(còn nữa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
